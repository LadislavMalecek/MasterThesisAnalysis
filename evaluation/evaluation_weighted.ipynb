{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the algorithms\n",
    "- we now have the matrix factorization model that is telling us our true ratings\n",
    "- together with groups and predicted top items for each group and each algorithm\n",
    "\n",
    "- we can now evaluate the performance of the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "sys.path.append(os.path.join(sys.path[0], '..'))\n",
    "\n",
    "from evaluation.evaluation_utils import calculate_dcg, RatingsRetriever, load_mf_matrices, load_groups, get_top_dict, get_group_size_from_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class WeightedMetrics:\n",
    "    avg_ratings: List[float] = field(default_factory=list)\n",
    "    corr_ratings: List[float] = field(default_factory=list)\n",
    "    mae_ratings: List[float] = field(default_factory=list)\n",
    "\n",
    "    avg_ndcgs_ratings: List[float] = field(default_factory=list)\n",
    "    corr_ndcgs_ratings: List[float] = field(default_factory=list)\n",
    "    mae_ndcgs_ratings: List[float] = field(default_factory=list)\n",
    "\n",
    "    alg_name: str = field(default=None)\n",
    "    group_name: str = field(default=None)\n",
    "\n",
    "    def to_avg_dict(self):\n",
    "        return {\n",
    "            'alg_name': self.alg_name,\n",
    "            'group_name': self.group_name,\n",
    "            'avg_ratings': np.mean(self.avg_ratings),\n",
    "            'corr_ratings': np.mean(self.corr_ratings),\n",
    "            'mae_ratings': np.mean(self.mae_ratings),\n",
    "            \n",
    "            'avg_ndcgs_ratings': np.mean(self.avg_ndcgs_ratings),\n",
    "            'corr_ndcgs_ratings': np.mean(self.corr_ratings),\n",
    "            'mae_ndcgs_ratings': np.mean(self.mae_ndcgs_ratings),\n",
    "        }\n",
    "\n",
    "\n",
    "def calculate_metrics(groups, results, weights, ratings_retriever, idcg_top_k):\n",
    "    # now for each group (set of users), and items that have been recommended to the group\n",
    "    # we calculate, for each user, sum of ratings and ndcg of ratings\n",
    "    metrics = WeightedMetrics()\n",
    "\n",
    "    # groups is an np.array of groups for each row we have n user idx which are the group members\n",
    "    # results is an np.array of items for each group\n",
    "    # weights is an np.array of weights for each group\n",
    "\n",
    "    for group, result, weight in tqdm(list(zip(groups.values, results, weights))):\n",
    "        group_item_ratings = ratings_retriever.get_ratings(group, result)\n",
    "        ratings_user_sum = np.sum(group_item_ratings, axis=1)\n",
    "\n",
    "        dcgs = np.apply_along_axis(calculate_dcg, 1, group_item_ratings)\n",
    "        idcgs = np.array([ratings_retriever.get_user_IDCG(user_id, idcg_top_k) for user_id in group])\n",
    "        ndcgs = dcgs / idcgs\n",
    "\n",
    "        normalized_ratings = ratings_user_sum / np.sum(ratings_user_sum)\n",
    "        normalized_ndcgs = ndcgs / np.sum(ndcgs)\n",
    "\n",
    "        metrics.avg_ratings.append(float(np.mean(ratings_user_sum)))\n",
    "        metrics.corr_ratings.append(pearsonr(normalized_ratings, weight)[0])\n",
    "        metrics.mae_ratings.append(mean_absolute_error(normalized_ratings, weight))\n",
    "\n",
    "        metrics.avg_ndcgs_ratings.append(float(np.mean(ndcgs)))\n",
    "        metrics.corr_ndcgs_ratings.append(pearsonr(normalized_ndcgs, weight)[0])\n",
    "        metrics.mae_ndcgs_ratings.append(mean_absolute_error(normalized_ndcgs, weight))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def process_results(mf_path, groups_path, groups_weights_path, results_path, idcg_top_k):\n",
    "    u_features, i_features = load_mf_matrices(mf_path)\n",
    "    ratings_retriever = RatingsRetriever(u_features, i_features)\n",
    "    groups = load_groups(groups_path)\n",
    "\n",
    "    # now, for each group type we have results for each algorithm\n",
    "    results = []\n",
    "    for group_name in sorted(os.listdir(results_path)):\n",
    "        # skip if not directory\n",
    "        if not os.path.isdir(os.path.join(results_path, group_name)):\n",
    "            continue\n",
    "        group_size = get_group_size_from_name(group_name)\n",
    "        #load weights csv to np\n",
    "        weights = np.loadtxt(os.path.join(groups_weights_path, f'group_weights_{group_size}.csv'), delimiter=',')\n",
    "\n",
    "        print(f'--- processing group: {group_name}')\n",
    "        group_results = {}\n",
    "        for result_file in os.listdir(os.path.join(results_path, group_name)):\n",
    "            # if result_file != 'avg_uniform.npy':\n",
    "                # continue\n",
    "            result = np.load(os.path.join(results_path, group_name, result_file))\n",
    "            \n",
    "            algorithm_name = result_file.split('.')[0]\n",
    "            # print(result_file)\n",
    "            metrics = calculate_metrics(\n",
    "                groups[group_name],\n",
    "                result,\n",
    "                weights=weights,\n",
    "                ratings_retriever=ratings_retriever,\n",
    "                idcg_top_k=idcg_top_k,\n",
    "            )\n",
    "            metrics.alg_name = algorithm_name\n",
    "            metrics.group_name = group_name\n",
    "\n",
    "            results.append(metrics)\n",
    "    avg_results = pd.DataFrame([result.to_avg_dict() for result in results])\n",
    "    avg_results['group_size'] = avg_results['group_name'].apply(get_group_size_from_name)\n",
    "    return avg_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_type_dict = {\n",
    "    'avg_ratings': 'max',\n",
    "    'corr_ratings': 'max',\n",
    "    'mae_ratings': 'min',\n",
    "    'avg_ndcgs_ratings': 'max',\n",
    "    'corr_ndcgs_ratings': 'max',\n",
    "    'mae_ndcgs_ratings': 'min',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latex_table_data(metrics: WeightedMetrics, all_other_metrics: List[WeightedMetrics]):\n",
    "    res_order = ['avg_ratings', 'corr_ratings', 'mae_ratings', 'avg_ndcgs_ratings', 'corr_ndcgs_ratings', 'mae_ndcgs_ratings']\n",
    "    top_dict = get_top_dict(all_other_metrics, sort_type_dict)\n",
    "    texts = []\n",
    "    for res_name in res_order:\n",
    "        possition = top_dict[res_name].tolist().index(metrics.alg_name)\n",
    "        if possition == 0:\n",
    "            texts.append(f'\\\\textbf{{{metrics[res_name]:.2f}}}')\n",
    "        elif possition == 1:\n",
    "            texts.append(f'\\\\underline{{{metrics[res_name]:.2f}}}')\n",
    "        else:\n",
    "            texts.append(f'{metrics[res_name]:.2f}')\n",
    "    return ' & '.join(texts)\n",
    "\n",
    "def create_latex_table(avg_results, eval_path, dataset_name):\n",
    "    map_alg_name = {\n",
    "        'avg_uniform': 'AVG-U',\n",
    "        'avg': 'AVG',\n",
    "        'dhondt_do': 'DHondtDO',\n",
    "        'ep_fuzz_dhondt': 'EP-Fuzz-DA',\n",
    "    }\n",
    "    alg_order = ['avg_uniform', 'avg', 'dhondt_do', 'ep_fuzz_dhondt']\n",
    "\n",
    "    output_lines = []\n",
    "    output_lines.append('\\\\begin{tabular}{ c | c c c | c c c || c c c | c c c}')\n",
    "    for i, group_size in enumerate([4, 6, 8]):\n",
    "        group_order = [f'prs_{group_size}_se=1_noc=1000', f'prs_{group_size}_se=4_noc=1000']\n",
    "        output_lines.append('')\n",
    "        if i != 0:\n",
    "            output_lines.append('\\multicolumn{12}{c}{} \\\\\\\\')\n",
    "        # print('& \\multicolumn{12}{c}{\\\\textbf{group size ' + str(group_size) +'}} \\\\\\\\')\n",
    "        output_lines.append('\\multicolumn{1}{c}{} & \\multicolumn{6}{c}{PRS(M=1)' + f', group size s={group_size}' + '} & \\multicolumn{6}{c}{PRS(M=4)' + f', group size s={group_size}' + '} \\\\\\\\')\n",
    "        output_lines.append('\\multicolumn{1}{c}{} & \\multicolumn{3}{c}{AR} & \\multicolumn{3}{c}{nDCG} & \\multicolumn{3}{c}{AR} & \\multicolumn{3}{c}{nDCG} \\\\\\\\')\n",
    "        output_lines.append('& mean & corr & MAE & mean & corr & MAE & mean & corr & MAE & mean & corr & MAE \\\\\\\\')\n",
    "        output_lines.append('\\hline')\n",
    "        for alg in alg_order:\n",
    "            alg_texts = []\n",
    "            for group in group_order:\n",
    "                all_metrics_for_group = avg_results[avg_results['group_name'] == group]\n",
    "                specific_results: WeightedMetrics = all_metrics_for_group[all_metrics_for_group['alg_name'] == alg].iloc[0]\n",
    "                # print(specific_results)\n",
    "                # print in the order\n",
    "                ltx_table_data = get_latex_table_data(specific_results, all_metrics_for_group)\n",
    "                alg_texts.append(ltx_table_data)\n",
    "            \n",
    "            output_lines.append(f'{map_alg_name[alg]} & {\" & \".join(alg_texts)} \\\\\\\\')\n",
    "    output_lines.append('')\n",
    "    output_lines.append('\\end{tabular}')\n",
    "\n",
    "    # make sure the directory exists\n",
    "    os.makedirs(eval_path, exist_ok=True)\n",
    "    # write the lines to file in results_evaluation\n",
    "    results_file_path = os.path.join(eval_path, f'{dataset_name}_weighted_results.tex')\n",
    "    with open(results_file_path, 'w') as f:\n",
    "        f.writelines(line + '\\n' for line in output_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    'kgrec',\n",
    "    # 'movie_lens',\n",
    "    # 'spotify',\n",
    "    # 'netflix'\n",
    "]\n",
    "\n",
    "for dataset in datasets:\n",
    "    data_dir = f'../datasets/{dataset}/'\n",
    "    \n",
    "    mf_path = os.path.join(data_dir, 'mf')\n",
    "    groups_path = os.path.join(data_dir, 'groups')\n",
    "    groups_weights_path = os.path.join(groups_path, 'weights')\n",
    "    results_path = os.path.join(data_dir, 'experiment_results', 'weighted')\n",
    "\n",
    "    eval_path = os.path.join(data_dir, 'evaluation_results')\n",
    "\n",
    "    idcg_top_k = 10\n",
    "\n",
    "    results = process_results(mf_path, groups_path, groups_weights_path, results_path, idcg_top_k)\n",
    "\n",
    "    os.makedirs(eval_path, exist_ok=True)\n",
    "    results.to_csv(os.path.join(eval_path, f'{dataset}_weighted_results.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    data_dir = f'../datasets/{dataset}/'\n",
    "    eval_path = os.path.join(data_dir, 'evaluation_results')\n",
    "    results = pd.read_csv(os.path.join(eval_path, f'{dataset}_weighted_results.csv'), index_col=False)\n",
    "    print('create_latex_table')\n",
    "    create_latex_table(results, eval_path, dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b9e6d427c7ffb7051e7a4a630c7ee86c58c59453b387f56f3562822577aaf7f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
