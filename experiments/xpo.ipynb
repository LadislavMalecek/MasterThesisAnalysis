{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "u_features = np.load('../datasets/movie_lens/mf/U_features.npy')\n",
    "i_features = np.load('../datasets/movie_lens/mf/I_features.npy')\n",
    "print(u_features.shape)\n",
    "print(i_features.shape)\n",
    "\n",
    "def get_items_for_user(user_id):\n",
    "    items_ratings = u_features[:, user_id] @ i_features\n",
    "    items_ids_w_ratings = [(item_id, rating) for item_id, rating in enumerate(items_ratings)]\n",
    "    items_ids_w_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "    return items_ids_w_ratings\n",
    "\n",
    "def get_items_for_users(users_id: List):\n",
    "    items_ratings = np.minimum(5, np.maximum(0, i_features.T @ u_features[:, users_id]))\n",
    "    return items_ratings\n",
    "    \n",
    "ratings = get_items_for_users([10,20,30])\n",
    "ratings.shape\n",
    "\n",
    "def select_top_n_idx(score_list, top_n, top='max', sort=True, exclude_idx=[]):\n",
    "    if top != 'max' and top != 'min':\n",
    "        raise ValueError('top must be either Max or Min')\n",
    "    if top == 'max':\n",
    "        score_list = -score_list\n",
    "\n",
    "    select_top_n = top_n + len(exclude_idx)\n",
    "    top_n_ind = np.argpartition(score_list, select_top_n)[:select_top_n]\n",
    "\n",
    "    if sort:\n",
    "        top_n_ind = top_n_ind[np.argsort(score_list[top_n_ind])]\n",
    "\n",
    "    if exclude_idx:\n",
    "        top_n_ind = [idx for idx in top_n_ind if idx not in exclude_idx]\n",
    "    return top_n_ind[0:top_n]\n",
    "\n",
    "\n",
    "a = np.array([2,1,6,7,8,9,3,4,5,10])\n",
    "assert np.array_equal(select_top_n_idx(a, 3, top='max'), [9, 5, 4])\n",
    "assert np.array_equal(select_top_n_idx(a, 3, top='min'), [1, 0, 6])\n",
    "assert set(select_top_n_idx(a, 3, top='max', sort=False)) == {9, 5, 4}\n",
    "assert set(select_top_n_idx(a, 3, top='min', sort=False)) == {0, 1, 6}\n",
    "\n",
    "assert np.array_equal(select_top_n_idx(a, 3, top='max', exclude_idx=[1]), [9, 5, 4])\n",
    "assert np.array_equal(select_top_n_idx(a, 3, top='min', exclude_idx=[1]), [0, 6, 7])\n",
    "assert set(select_top_n_idx(a, 3, top='max', sort=False, exclude_idx=[1])) == {9, 5, 4}\n",
    "assert set(select_top_n_idx(a, 3, top='min', sort=False, exclude_idx=[1])) == {0, 6, 7}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import rankdata\n",
    "\n",
    "def get_rank(candidate_group_items_np):\n",
    "    return rankdata(-candidate_group_items_np, method='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xpo_algorithm(group_items, top_n: int, n_candidates:int, type:str='XPO', mc_trials:int=1000):\n",
    "    if type != 'XPO' and type != 'NPO':\n",
    "        raise ValueError(f'Unknown type: {type}, only XPO and NPO are supported')\n",
    "\n",
    "    group_size = group_items.shape[1]\n",
    "    # first select top candidates for each member\n",
    "    top_candidates_ids_per_member = np.apply_along_axis(lambda u_items: select_top_n_idx(u_items, n_candidates, sort=False), 0, group_items)\n",
    "    # these are the original items ids\n",
    "    top_candidates_idx = np.array(sorted(set(top_candidates_ids_per_member.flatten())))\n",
    "\n",
    "    # get the candidate group items for each member\n",
    "    candidate_group_items = group_items[top_candidates_idx, :] # this is the first id mapping (to go back to original, index by top_candidates_idx)\n",
    "\n",
    "    # from now on, item ids are ids into this candidate group items list\n",
    "\n",
    "    # calculate the rank of candidates and all values bigger than n_candidates are set to n_candidates + 1\n",
    "    # ranks will therefore be [1, 101]\n",
    "    rank_of_candidates = np.apply_along_axis(get_rank, 0, candidate_group_items)\n",
    "    rank_of_candidates = np.minimum(rank_of_candidates, n_candidates + 1)\n",
    "\n",
    "    # now we want to compare all pairs of items to calculate the their Pareto optimality level\n",
    "    # we say that item a is pareto optimal over b if for all group members rank(a) <= rank(b)\n",
    "    # they can be both pareto optimal(if their ranks are equal)\n",
    "    # or non-pareto optimal(if their dominate each other for different group members)\n",
    "    number_of_items = len(candidate_group_items)\n",
    "    # print(f'Number of items: {number_of_items}')\n",
    "    pareto_levels = [1] * number_of_items # same ids as candidate_group_items\n",
    "    for a, b in itertools.combinations(range(len(candidate_group_items)) , 2):\n",
    "        rank_of_candidates_a = rank_of_candidates[a]\n",
    "        rank_of_candidates_b = rank_of_candidates[b]\n",
    "\n",
    "        rank_dif = rank_of_candidates_a - rank_of_candidates_b\n",
    "        # for each group member, if a is pareto optimal over b, then all rank_difs will be negative or zero\n",
    "        # if b is pareto optimal over 1, then all rank_difs will be positive or zero\n",
    "        # and atleast one rank for any group member will negative or positive (non-zero) respectively\n",
    "        a_paretooptim = np.all(rank_dif <= 0) & np.any(rank_dif < 0)\n",
    "        b_paretooptim = np.all(rank_dif >= 0) & np.any(rank_dif > 0)\n",
    "        # the lower the level the better, as an item, if you are bester (dominated) we lower your level by 1\n",
    "        # where level 1 is best and level infinity is worst\n",
    "        if a_paretooptim:\n",
    "            pareto_levels[b] += 1\n",
    "        if b_paretooptim:\n",
    "            pareto_levels[a] += 1\n",
    "\n",
    "    # now group the items by pareto level\n",
    "    pareto_levels_pd = pd.DataFrame(enumerate(pareto_levels), columns=['item_id', 'pareto_level'])\n",
    "    pareto_levels_pd = pareto_levels_pd.sort_values(by='pareto_level', ascending=True)\n",
    "\n",
    "    # select candidates by pareto level, cut off at pareto_level = top_n\n",
    "    if type == 'NPO':\n",
    "        final_candidates = pareto_levels_pd[pareto_levels_pd['pareto_level'] <= top_n]['item_id'].explode().to_numpy()\n",
    "\n",
    "    # select candidates starting at pareto level 1 and cut off when the number of candidates is equal or greater than top_n\n",
    "    if type == 'XPO':\n",
    "        # add cumulative count of items to the grouped dataframe\n",
    "        pareto_levels_grouped = pareto_levels_pd.groupby('pareto_level').agg(level=('pareto_level', 'first'), items=('item_id','unique'), items_count=('item_id','count'))\n",
    "        pareto_levels_grouped['cum_items_count'] = pareto_levels_grouped['items_count'].cumsum()\n",
    "        idx_of_first_larger_than_top_k = (pareto_levels_grouped['cum_items_count'] > top_n).idxmax()\n",
    "        # select all pareto levels that will get us top_n items and explode the lists to get the top_n items\n",
    "        final_candidates = pareto_levels_grouped.iloc[0:idx_of_first_larger_than_top_k]['items'].explode().to_numpy(dtype=np.int64)\n",
    "\n",
    "    final_candidates_group_items = group_items[final_candidates, :] # second id mapping (to go back index by final_candidates)\n",
    "    \n",
    "    # simple Monte Carlo method for computing probabilities of linear aggregation strategy ratios (analytical solution not feasible)\n",
    "    # now we want to try many different weights of users and award points to winners based on the weighted ranks\n",
    "\n",
    "    mc_score = np.zeros(len(final_candidates))\n",
    "\n",
    "    for i in range(mc_trials):\n",
    "        # get random weights summing up to 1 for the group members\n",
    "        weights = np.random.random(group_size)\n",
    "        weights /= weights.sum()\n",
    "        \n",
    "        group_items_score = np.sum(weights * final_candidates_group_items, axis=1)\n",
    "        top_n_idx = select_top_n_idx(group_items_score, top_n, sort=False)\n",
    "        mc_score[top_n_idx] += 1\n",
    "\n",
    "    # print(f'MC score: {mc_score}')\n",
    "    top_n_idx = select_top_n_idx(mc_score, top_n)\n",
    "    # print(top_n_idx)\n",
    "    # print(final_candidates[top_n_idx])\n",
    "    # now we need to get the original item ids from the final_candidates list and then top_candidates_idx\n",
    "    final_top_candidates = top_candidates_idx[final_candidates[top_n_idx]]\n",
    "    # print(f'Final top candidates: {final_top_candidates}')\n",
    "    return final_top_candidates\n",
    "\n",
    "\n",
    "group_size = 5\n",
    "\n",
    "# load groups\n",
    "groups = pd.read_csv('../notebooks/dfs/groups/kgrec/top_k_10.csv')\n",
    "#concatenate first 5 columns to array of ints\n",
    "groups = groups.iloc[:,:group_size].values\n",
    "rec_it = []\n",
    "\n",
    "for group_members in tqdm(groups):\n",
    "    items = get_items_for_users(group_members)\n",
    "\n",
    "    # xpo_algorithm\n",
    "    top_n_items_xpo = xpo_algorithm(items, 10, 100, type='XPO')\n",
    "    rec_it.append(top_n_items_xpo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "72e4c8b5f0869b81e3c54c1a9c17a5176fed7dccc000e70ae85e6cab596ae0d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
